{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNclyqQAyeSH2a6QdfOHiPt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "j37HIO_-KoIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1kD6ZqCKlcP"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/project/archive.zip -d /content/gtsrb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Set the base path to your dataset directory\n",
        "base_path = '/content/gtsrb'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CQo0YmoqMygU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List all directories within the train path (each class directory)\n",
        "class_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
        "\n",
        "# Iterate through each class directory and create a 'labels' subdirectory\n",
        "for class_id in class_dirs:\n",
        "    labels_path = os.path.join(base_path, class_id, 'labels')\n",
        "    os.makedirs(labels_path, exist_ok=True)\n",
        "    print(f\"Created or verified labels folder: {labels_path}\")\n"
      ],
      "metadata": {
        "id": "DutmOLFhPOfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "def split_data_and_csv(source, train_dest, valid_dest, csv_path, split_ratio=0.2):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Create mappings from image filenames to csv rows\n",
        "    file_to_row = {row['Path']: row for index, row in df.iterrows()}\n",
        "\n",
        "    # Create new dataframes for train and valid splits\n",
        "    train_df = pd.DataFrame(columns=df.columns)\n",
        "    valid_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    # For each class directory in the source\n",
        "    for class_dir in os.listdir(source):\n",
        "        class_path = os.path.join(source, class_dir)\n",
        "        images = os.listdir(class_path)\n",
        "        np.random.shuffle(images)  # Shuffle the list of images\n",
        "\n",
        "        # Split into train and validation\n",
        "        split_point = int(len(images) * (1 - split_ratio))\n",
        "        valid_images = images[split_point:]\n",
        "        train_images = images[:split_point]\n",
        "\n",
        "        # Ensure target directories exist\n",
        "        os.makedirs(os.path.join(train_dest, class_dir), exist_ok=True)\n",
        "        os.makedirs(os.path.join(valid_dest, class_dir), exist_ok=True)\n",
        "\n",
        "        # Move files and divide CSV rows\n",
        "        for image in train_images:\n",
        "            shutil.move(os.path.join(class_path, image), os.path.join(train_dest, class_dir))\n",
        "            train_df = pd.concat([train_df, df[df['Path'] == image]], ignore_index=True)\n",
        "\n",
        "        for image in valid_images:\n",
        "            shutil.move(os.path.join(class_path, image), os.path.join(valid_dest, class_dir))\n",
        "            valid_df = pd.concat([valid_df, df[df['Path'] == image]], ignore_index=True)\n",
        "\n",
        "    # Save new CSV files\n",
        "    train_df.to_csv(os.path.join(train_dest, 'train_labels.csv'), index=False)\n",
        "    valid_df.to_csv(os.path.join(valid_dest, 'valid_labels.csv'), index=False)\n",
        "\n",
        "# Define paths\n",
        "base_path = '/content/gtsrb'\n",
        "source_path = os.path.join(base_path, 'train')  # Adjust if different\n",
        "train_path = os.path.join(base_path, 'train_new')\n",
        "valid_path = os.path.join(base_path, 'valid_new')\n",
        "csv_path = os.path.join(base_path, 'Train.csv')  # Path to your full CSV file\n",
        "\n",
        "# Execute the function\n",
        "split_data_and_csv(source_path, train_path, valid_path, csv_path)\n"
      ],
      "metadata": {
        "id": "BNhVJMQVX833"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming CSV annotations are available\n",
        "# GTSRB typically comes with CSV files that list image names, bounding boxes, and class labels\n",
        "annotations_csv_train = pd.read_csv('/content/gtsrb/train_new/train_labels.csv')\n",
        "annotations_csv_valid = pd.read_csv('/content/gtsrb/valid_new/valid_labels.csv')\n",
        "\n",
        "for _, row in annotations_csv_train.iterrows():\n",
        "    # Construct the image path\n",
        "    img_path = os.path.join(base_path, row['Path'])\n",
        "    image = Image.open(img_path)\n",
        "    iw, ih = image.size\n",
        "\n",
        "    # Normalize the bounding box dimensions\n",
        "    x_center = (row['Roi.X1'] + (row['Roi.X2'] - row['Roi.X1']) / 2) / iw\n",
        "    y_center = (row['Roi.Y1'] + (row['Roi.Y2'] - row['Roi.Y1']) / 2) / ih\n",
        "    width = (row['Roi.X2'] - row['Roi.X1']) / iw\n",
        "    height = (row['Roi.Y2'] - row['Roi.Y1']) / ih\n",
        "\n",
        "    # Format for YOLO\n",
        "    yolo_data = f\"{row['ClassId']} {x_center} {y_center} {width} {height}\\n\"\n",
        "\n",
        "    # Write to the corresponding .txt file\n",
        "    txt_path = os.path.join(base_path, 'train_new/labels', os.path.splitext(os.path.basename(row['Path']))[0] + '.txt')\n",
        "    with open(txt_path, 'a') as file:\n",
        "        file.write(yolo_data)\n",
        "\n",
        "for _, row in annotations_csv_valid.iterrows():\n",
        "    # Construct the image path\n",
        "    img_path = os.path.join(base_path, row['Path'])\n",
        "    image = Image.open(img_path)\n",
        "    iw, ih = image.size\n",
        "\n",
        "    # Normalize the bounding box dimensions\n",
        "    x_center = (row['Roi.X1'] + (row['Roi.X2'] - row['Roi.X1']) / 2) / iw\n",
        "    y_center = (row['Roi.Y1'] + (row['Roi.Y2'] - row['Roi.Y1']) / 2) / ih\n",
        "    width = (row['Roi.X2'] - row['Roi.X1']) / iw\n",
        "    height = (row['Roi.Y2'] - row['Roi.Y1']) / ih\n",
        "\n",
        "    # Format for YOLO\n",
        "    yolo_data = f\"{row['ClassId']} {x_center} {y_center} {width} {height}\\n\"\n",
        "\n",
        "    # Write to the corresponding .txt file\n",
        "    txt_path = os.path.join(base_path, 'valid_new/labels', os.path.splitext(os.path.basename(row['Path']))[0] + '.txt')\n",
        "    with open(txt_path, 'a') as file:\n",
        "        file.write(yolo_data)\n"
      ],
      "metadata": {
        "id": "oMukuIosPRkM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}